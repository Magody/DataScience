{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/magody/anaconda3/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "# relevant packages & modules\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import requests\n",
    "import urllib.request\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "# from tqdm.notebook import tqdm\n",
    "import undetected_chromedriver.v2 as uc\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Have powerfull bot protection, so we need undetected_chrome\n",
    "class ScrapperPlusvalia:\n",
    "    \n",
    "    driver = None\n",
    "\n",
    "    \n",
    "    logged_in = False\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.driver = uc.Chrome(version_main=95)\n",
    "        self.root = \"https://www.plusvalia.com/inmuebles-ordenado-por-fechaonline-descendente\"\n",
    "        self.base = f\"{self.root}-pagina-\"\n",
    "        self.driver.get(f\"{self.base}1.html\")\n",
    "\n",
    "    def wait(self, t):\n",
    "        time.sleep(t)\n",
    "        \n",
    "    def check_requisites(self):     \n",
    "        return True\n",
    "    \n",
    "    def collect_urls(self,page_from,page_to,waiting_time=2)->list:\n",
    "        \n",
    "        if not self.check_requisites():\n",
    "            return []\n",
    "\n",
    "        urls = []\n",
    "        repeated = []\n",
    "        \n",
    "\n",
    "        with open(\"urls.txt\",\"a\") as f:\n",
    "\n",
    "            for page_i in tqdm(range(page_from, page_to+1)):\n",
    "\n",
    "                url = f\"{self.base}{page_i}.html\"\n",
    "                self.driver.get(url)\n",
    "                \n",
    "                chunk = self.driver.find_elements(By.XPATH, '//div[contains(@class,\"list-card-container\")]/div')\n",
    "                \n",
    "                for c in chunk:\n",
    "                    if str(c.get_attribute(\"class\")) != \"ads-container\":\n",
    "\n",
    "                        url_item = str(c.get_attribute(\"data-to-posting\"))\n",
    "                        if url_item is None:\n",
    "                            pass\n",
    "                        else:\n",
    "                            if url_item not in urls:\n",
    "                                urls.append(url_item)\n",
    "                            else:\n",
    "                                repeated.append(f\"Repeated inmuebles-ordenado-por-fechaonline-descendente-{page_i}.html:{url_item}\")\n",
    "\n",
    "                if len(urls) > 0:\n",
    "                    f.write(\"\\n\".join(map(lambda s: f\"https://www.plusvalia.com{s}\", urls)))\n",
    "                    urls = []\n",
    "\n",
    "                current_url = self.driver.current_url\n",
    "\n",
    "\n",
    "                if current_url == f\"{self.root}.html\":\n",
    "                    page = 1\n",
    "                else:\n",
    "                    page = int(re.match(r\"^https\\:\\/\\/www\\.plusvalia\\.com\\/[A-z\\-]*(\\d+)\\.html$\", current_url).groups()[0])\n",
    "\n",
    "                if page == page_to:\n",
    "                    break\n",
    "                \n",
    "                # next = self.driver.find_elements(By.CSS_SELECTOR, 'li.pag-go-next > a')[0]\n",
    "                # next.click()\n",
    "                self.wait(waiting_time)\n",
    "\n",
    "        return urls, repeated\n",
    "\n",
    "\n",
    "    def scrape_items(self,urls_items)->list:\n",
    "        \n",
    "        if not self.check_requisites():\n",
    "            return []\n",
    "\n",
    "        \n",
    "    \n",
    "    def close(self):\n",
    "        if self.driver:\n",
    "            self.driver.close()\n",
    "            \n",
    "    def __del__(self):\n",
    "        print(\"Closing...\")\n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapper = ScrapperPlusvalia()\n",
    "# urls, _ = scrapper.collect_urls(1,3468,1.5)  # 3468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36785\n",
      "17750\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "with open(\"urls.txt\") as f:\n",
    "    lines = f.readlines()\n",
    "    print(len(lines))\n",
    "    lines = np.unique(lines)\n",
    "    print(len(lines))\n",
    "\n",
    "    with open(\"urls_clean.txt\",\"w\") as f_out:\n",
    "        f_out.writelines(map(lambda s: s, lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_first(elements, mapping=None, default=\"none\"):\n",
    "    elements_len = len(elements)\n",
    "    if elements_len == 0:\n",
    "        return default\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        element = elements[0].text\n",
    "        if mapping is not None:\n",
    "            element = mapping(element)\n",
    "        return element\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "\n",
    "def parse_multiple(web_elements, separator=\"|\"):\n",
    "    \n",
    "    out = \"\"\n",
    "    for i in range(len(web_elements)):\n",
    "        out += web_elements[i].text\n",
    "        if i < len(web_elements)-1:\n",
    "            out += separator        \n",
    "    return out\n",
    "    \n",
    "class ItemRealState:\n",
    "    \n",
    "    attributes = []\n",
    "    \n",
    "    def set_attributes(self, dict_attributes:dict):\n",
    "        self.attributes = []\n",
    "        for key,value in dict_attributes.items():\n",
    "            setattr(self, key, value)\n",
    "            self.attributes.append(key)\n",
    "    \n",
    "    def get_serie(self, map_attributes_to_columns=dict()):\n",
    "        if len(self.attributes) == 0:\n",
    "            print(\"First add atributes with set_attributes\")\n",
    "            return pd.Series()\n",
    "        \n",
    "        if len(map_attributes_to_columns) > 0:\n",
    "            # not supported\n",
    "            return pd.Series()\n",
    "        \n",
    "        data = dict()\n",
    "        for attribute in self.attributes:\n",
    "            data[attribute] = getattr(self, attribute)\n",
    "        \n",
    "        return pd.Series(data=data)\n",
    "            \n",
    "\"\"\"\n",
    "### REFERENCES\n",
    "https://www.plusvalia.com/propiedades/vendo-terreno-con-casa-valle-de-los-chillos-sector-60551153.html\n",
    "https://www.plusvalia.com/propiedades/venta-en-remate-de-hermosa-casa-en-ibarra-ahora-63081714.html\n",
    "https://www.plusvalia.com/propiedades/alquiler-departamento-sector-gonzalez-suarez-60929588.html\n",
    "https://www.plusvalia.com/casas-en-venta-ordenado-por-precioxm2-descendente.html#\n",
    "https://www.plusvalia.com/propiedades/vendo-o-arriendo-casa-amoblada-urbanizacion-cerrada-63007741.html\n",
    "https://www.plusvalia.com/casas-en-venta-ordenado-por-cantidadvisitas-descendente.html#\n",
    "https://www.plusvalia.com/propiedades/zaigen-oficinas-y-locales.-sector-plataforma-55554056.html?labs=developments&userid=null&itemid=62559412&labs_source=RECOMENDADOS_FICHA_PROPIEDAD_DESKTOP\n",
    "\n",
    "\n",
    "self.cache[\"urls\"] = urls_items\n",
    "self.cache[\"index_seen\"] = -1\n",
    "self.cache[\"urls_error\"] = []\n",
    "self.cache[\"df\"] = df\n",
    "\n",
    "for line in [\"https://www.plusvalia.com/propiedades/casa-calderon-63208500.html\"]:\n",
    "# line = \"https://www.plusvalia.com/propiedades/casa-calderon-63208500.html\"\n",
    "scrapper.driver.get(line)\n",
    "scrapper.wait(1)\n",
    "\"\"\"\n",
    "import datetime\n",
    "##\n",
    "restarting = False\n",
    "cache = dict()\n",
    "stop = False\n",
    "##\n",
    "today = datetime.date.today()\n",
    "\n",
    "def dateToString(date):\n",
    "    return f\"{date.day}/{date.month}/{date.year}\"\n",
    "today_str = dateToString(today)\n",
    "\n",
    "web_elements_location_tree = scrapper.driver.find_elements(By.XPATH, \"//ul[@class='breadcrumb']/li\")\n",
    "\n",
    "location_tree = parse_multiple(web_elements_location_tree,\"->\")\n",
    "\n",
    "type_real_state = web_elements_location_tree[1].text.strip().lower()\n",
    "\n",
    "if type_real_state == \"casa\":\n",
    "    title = extract_first(scrapper.driver.find_elements(By.XPATH, \"//div[@class='section-title']\"))\n",
    "    location_string = extract_first(scrapper.driver.find_elements(By.XPATH, \"//h2[@class='title-location']\")).replace(\"\\nVer en mapa\", \"\")\n",
    "    \n",
    "    url_maps = scrapper.driver.find_elements(By.XPATH, \"//img[@id='static-map']\")[0].get_attribute(\"src\")\n",
    "    location_latitude, location_longitude = re.match(r\".*markers=(\\-?\\d+\\.[\\d]+),(\\-?\\d+\\.[\\d]+).*\",url_maps).groups()\n",
    "    \n",
    "    date_publication = extract_first(scrapper.driver.find_elements(By.XPATH, \"//div[@id='user-views']/div/div[1]\"))\n",
    "    views = int(extract_first(scrapper.driver.find_elements(By.XPATH, \"//div[@id='user-views']/div/div[2]\"),default=\"0 visualizaciones\").split(\" \")[0])\n",
    "    if date_publication == \"Publicado hoy\":\n",
    "        date_publication = today_str\n",
    "    else:\n",
    "        days = int(re.match(\"Publicado hace (\\d+).*\", date_publication).groups()[0])\n",
    "        date_publication = dateToString(today - datetime.timedelta(days=days))\n",
    "\n",
    "    state = extract_first(scrapper.driver.find_elements(By.XPATH, \"//div[@class='price-operation']\"))\n",
    "    deliver = \"\"\n",
    "    price_items_split = extract_first(scrapper.driver.find_elements(By.XPATH, \"//div[@class='price-items']/span/span\")).split(\"\\n\")\n",
    "    coin, price = re.match(\"([A-z]+) ([\\d\\.]+)\", price_items_split[0]).groups()\n",
    "    price = float(price.replace(\".\",\"\"))\n",
    "    discount = 0 if len(price_items_split) == 1 else int(price_items_split[1].replace(\"%\",\"\"))\n",
    "    \n",
    "    description_sup = extract_first(scrapper.driver.find_elements(By.XPATH, \"//h2[@class='title-type-sup']\"))\n",
    "    description = extract_first(scrapper.driver.find_elements(By.XPATH, \"//div[@id='longDescription']\"))\n",
    "    \n",
    "    web_elements_imgs = scrapper.driver.find_elements(By.XPATH, \"//div[@id='react-gallery']//img\")\n",
    "    pictures = \"\"\n",
    "    for i,element in enumerate(web_elements_imgs):\n",
    "        pictures += element.get_attribute(\"src\")\n",
    "        if i < len(web_elements_imgs)-1:\n",
    "            pictures += \"|\"\n",
    "    \n",
    "    advertiser_name = extract_first(scrapper.driver.find_elements(By.XPATH, \"//section[@id='reactPublisherData']/div/div\"))\n",
    "    advertiser_codes = parse_multiple(scrapper.driver.find_elements(By.XPATH, \"//section[@id='reactPublisherData']/div/ul/li\"))\n",
    "\n",
    "    feature_unities = \"\"\n",
    "    feature_stotal = \"\"\n",
    "    feature_scover = \"\"\n",
    "    feature_bath = \"\"\n",
    "    feature_parking = \"\"\n",
    "    feature_toilete= \"\"\n",
    "    feature_antique= \"\"\n",
    "\n",
    "\n",
    "    extra_discover = \"\"\n",
    "else:\n",
    "    title = \"\"\n",
    "    location_string = \"\"\n",
    "    location_latlon = \"\"\n",
    "    date_publication = \"\"\n",
    "    state = \"\"\n",
    "    deliver = \"\"\n",
    "    price = \"\"\n",
    "    description_sup = \"\"\n",
    "    description = \"\"\n",
    "    pictures = \"\"\n",
    "    advertiser_code = \"\"\n",
    "    advertiser_code_plusvalia = \"\"\n",
    "    advertiser_lic= \"\"\n",
    "\n",
    "    feature_unities = \"\"\n",
    "    feature_stotal = \"\"\n",
    "    feature_scover = \"\"\n",
    "    feature_bath = \"\"\n",
    "    feature_parking = \"\"\n",
    "    feature_toilete= \"\"\n",
    "    feature_antique= \"\"\n",
    "\n",
    "\n",
    "    extra_discover = \"\"\n",
    "\n",
    "\n",
    "# \"location_tree\":location_tree, include\n",
    "attributes = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'USD 67.000\\n39%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cód. del anunciante: 2EMJ1N|Cód. Plusvalia: 60551153'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_multiple(scrapper.driver.find_elements(By.XPATH, \"//section[@id='reactPublisherData']/div/ul/li\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2022, 1, 21)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today  - datetime.timedelta(days=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3c2402a762b1da2b664ca9cbb9344946d41b73132102685c4db1aa6c02b5b44"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
