{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# clear_output()\n",
    "### LOGIN\n",
    "\n",
    "f = open(\".env.json\", \"r\")\n",
    "env = json.load(f)\n",
    "f.close()\n",
    "scrapper.login(env[\"username_animeflv\"], env[\"password_animeflv\"])\n",
    "del env[\"password_animeflv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ScrapperAnimeFLV.__del__ at 0x7f8d02c3f040>\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35982/4006993225.py\", line 364, in __del__\n",
      "  File \"/tmp/ipykernel_35982/4006993225.py\", line 359, in close\n",
      "  File \"/home/magody/anaconda3/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py\", line 937, in close\n",
      "    self.execute(Command.CLOSE)\n",
      "  File \"/home/magody/anaconda3/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py\", line 424, in execute\n",
      "    self.error_handler.check_response(response)\n",
      "  File \"/home/magody/anaconda3/lib/python3.8/site-packages/selenium/webdriver/remote/errorhandler.py\", line 247, in check_response\n",
      "    raise exception_class(message, screen, stacktrace)\n",
      "selenium.common.exceptions.InvalidSessionIdException: Message: invalid session id\n",
      "Stacktrace:\n",
      "#0 0x55cc5e322533 (/home/magody/.local/share/undetected_chromedriver/chromedriver (deleted)+0xffffffffffaeb532)\n",
      "#1 0x55cc5e08107f (/home/magody/.local/share/undetected_chromedriver/chromedriver (deleted)+0xffffffffff84a07e)\n",
      "#2 0x55cc5e0aa0ab (/home/magody/.local/share/undetected_chromedriver/chromedriver (deleted)+0xffffffffff8730aa)\n",
      "#3 0x55cc5e0d4ebc (/home/magody/.local/share/undetected_chromedriver/chromedriver (deleted)+0xffffffffff89debb)\n",
      "#4 0x55cc5e0d2b61 (/home/magody/.local/share/undetected_chromedriver/chromedriver (deleted)+0xffffffffff89bb60)\n",
      "#5 0x55cc5e0d2397 (/home/magody/.local/share/undetected_chromedriver/chromedriver (deleted)+0xffffffffff89b396)\n",
      "#6 0x55cc5e057eb4 (/home/magody/.local/share/undetected_chromedriver/chromedriver (deleted)+0xffffffffff820eb3)\n",
      "#7 0x55cc5e058d10 (/home/magody/.local/share/undetected_chromedriver/chromedriver (deleted)+0xffffffffff821d0f)\n",
      "#8 0x55cc5e366e1d (/home/magody/.local/share/undetected_chromedriver/chromedriver (deleted)+0xffffffffffb2fe1c)\n",
      "#9 0x55cc5e36a751 (/home/magody/.local/share/undetected_chromedriver/chromedriver (deleted)+0xffffffffffb33750)\n",
      "#10 0x55cc5e35107e (/home/magody/.local/share/undetected_chromedriver/chromedriver (deleted)+0xffffffffffb1a07d)\n",
      "#11 0x55cc5e36b388 (/home/magody/.local/share/undetected_chromedriver/chromedriver (deleted)+0xffffffffffb34387)\n",
      "#12 0x55cc5e345fe0 (/home/magody/.local/share/undetected_chromedriver/chromedriver (deleted)+0xffffffffffb0efdf)\n",
      "#13 0x55cc5e057a26 (/home/magody/.local/share/undetected_chromedriver/chromedriver (deleted)+0xffffffffff820a25)\n",
      "#14 0x7ff77987d0b3 <unknown>\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESTRUCTING OBJECT\n",
      "CLOSING OBJECT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged in!\n",
      "Pending work...100 urls in queue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [07:50<03:18,  6.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discuss error 1. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [09:51<01:28,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discuss error 1. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [11:21<00:00,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n",
      "scrape_items ended\n",
      "Stopping scraper...Completed: True DF type: <class 'pandas.core.frame.DataFrame'>\n",
      "Saving job...\n",
      "Closing with: 100 completed, 0 pending urls and 0 error urls\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "global job, file_job\n",
    "job_name = \"1\"\n",
    "\n",
    "folder_job = os.path.join(\"jobs\",job_name)\n",
    "file_job = os.path.join(folder_job,\"info.json\")\n",
    "\n",
    "if not os.path.isdir(folder_job):\n",
    "    \n",
    "    # sys.exit(0)\n",
    "else:\n",
    "    print(\"That job doesn't exist\")\n",
    "    \n",
    "\n",
    "def close_job(is_completed, df):\n",
    "    global scrapper\n",
    "    global file_job, job \n",
    "    print(\"Stopping scraper...\", is_completed, type(df))\n",
    "    scrapper.stop = True\n",
    "    \n",
    "    print(\"Saving job...\")\n",
    "    \n",
    "    pending = []\n",
    "    if not is_completed:\n",
    "        index_seen = scrapper.cache.get(\"index_seen\",-1)\n",
    "        pending = scrapper.cache.get(\"urls\",[])\n",
    "        if len(pending) > 0:\n",
    "            pending = pending[index_seen+1:]\n",
    "          \n",
    "        df = scrapper.cache.get(\"df\", None)\n",
    "        \n",
    "    job['urls_queued'] = pending\n",
    "    \n",
    "    job['urls_error'] = scrapper.cache.get(\"urls_error\", [])\n",
    "    save_job(file_job, job)\n",
    "    \n",
    "    if df is not None:\n",
    "        df.to_csv(f\"temp/db_{job['name']}_items{job['execution_time']}.csv\", index=False)\n",
    "    \n",
    "    print(f\"Closing with: {len(scrapper.cache.get('urls',[])) -len(job['urls_queued'])} completed, {len(job['urls_queued'])} pending urls and {len(job['urls_error'])} error urls\")\n",
    "    \n",
    "\n",
    "def signal_handler(sig, frame):\n",
    "    close_job(False, None)\n",
    "    \n",
    "    try:\n",
    "        sys.exit()\n",
    "    except:\n",
    "        print(\"Problems with sys.exit!\")\n",
    "        sys.exit()\n",
    "        \n",
    "signal.signal(signal.SIGINT, signal_handler)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "print(f\"Begin work, don't open or touch files in {folder_job}\")\n",
    "\n",
    "timers = {\n",
    "    \"timer_load_page\": 2,\n",
    "    \"timer_load_disqus\": 2,\n",
    "    \"timer_load_reactions\": 1\n",
    "}\n",
    "\n",
    "state = job[\"state\"]\n",
    "\n",
    "if state != State.END:\n",
    "    \n",
    "    f = open(\".env.json\", \"r\")\n",
    "    env = json.load(f)\n",
    "    f.close()\n",
    "    scrapper = ScrapperAnimeFLV(headless=False)  # \"/home/magody/chromedriver_linux64/chromedriver\"\n",
    "    scrapper.driver.get(\"https://www3.animeflv.net/\")\n",
    "    scrapper.wait(5)\n",
    "    # clear_output()\n",
    "    ### LOGIN\n",
    "    scrapper.login(env[\"username_animeflv\"], env[\"password_animeflv\"])\n",
    "    del env[\"password_animeflv\"]\n",
    "\n",
    "\n",
    "if state == State.BEGIN:\n",
    "    print(\"Begining\")\n",
    "    df = scrapper.scrape_items(timers, url_begin=\"https://www3.animeflv.net/browse?page=1\")\n",
    "    close_job(True, df)\n",
    "elif state == State.PENDING_ITEMS:\n",
    "    scrapper.stop = False\n",
    "    urls_items = job[\"urls_queued\"]\n",
    "    urls_items.extend(job[\"urls_error\"])  # try again the errors\n",
    "    job[\"urls_error\"] = []\n",
    "    job[\"execution_time\"] += 1\n",
    "    print(f\"Pending work...{len(urls_items)} urls in queue\")\n",
    "    df, completed = scrapper.scrape_items(timers, urls=urls_items)\n",
    "    print(\"scrape_items ended\")\n",
    "    if completed and len(scrapper.cache[\"urls_error\"]) == 0: \n",
    "        job[\"state\"] = State.END\n",
    "    close_job(completed, df)\n",
    "elif state == State.END:\n",
    "    print(\"Work already completed!\")\n",
    "else:\n",
    "    print(\"Can't handle this state\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLOSING OBJECT...\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3c2402a762b1da2b664ca9cbb9344946d41b73132102685c4db1aa6c02b5b44"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
