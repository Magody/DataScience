{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant packages & modules\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import requests\n",
    "import urllib.request\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "# from tqdm.notebook import tqdm\n",
    "import undetected_chromedriver.v2 as uc\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Have powerfull bot protection, so we need undetected_chrome\n",
    "class ScrapperPlusvalia:\n",
    "    \n",
    "    driver = None\n",
    "\n",
    "    \n",
    "    logged_in = False\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.driver = uc.Chrome(version_main=95)\n",
    "        self.root = \"https://www.plusvalia.com/inmuebles-ordenado-por-fechaonline-descendente\"\n",
    "        self.base = f\"{self.root}-pagina-\"\n",
    "        self.driver.get(f\"{self.base}1.html\")\n",
    "\n",
    "    def wait(self, t):\n",
    "        time.sleep(t)\n",
    "        \n",
    "    def check_requisites(self):     \n",
    "        return True\n",
    "    \n",
    "    def xp(self,xpath):\n",
    "        return self.driver.find_elements(By.XPATH, xpath)\n",
    "\n",
    "    def collect_urls(self,page_from,page_to,waiting_time=2)->list:\n",
    "        \n",
    "        if not self.check_requisites():\n",
    "            return []\n",
    "\n",
    "        urls = []\n",
    "        repeated = []\n",
    "        \n",
    "\n",
    "        with open(\"urls.txt\",\"a\") as f:\n",
    "\n",
    "            for page_i in tqdm(range(page_from, page_to+1)):\n",
    "\n",
    "                url = f\"{self.base}{page_i}.html\"\n",
    "                self.driver.get(url)\n",
    "                \n",
    "                chunk = self.driver.find_elements(By.XPATH, '//div[contains(@class,\"list-card-container\")]/div')\n",
    "                \n",
    "                for c in chunk:\n",
    "                    if str(c.get_attribute(\"class\")) != \"ads-container\":\n",
    "\n",
    "                        url_item = str(c.get_attribute(\"data-to-posting\"))\n",
    "                        if url_item is None:\n",
    "                            pass\n",
    "                        else:\n",
    "                            if url_item not in urls:\n",
    "                                urls.append(url_item)\n",
    "                            else:\n",
    "                                repeated.append(f\"Repeated inmuebles-ordenado-por-fechaonline-descendente-{page_i}.html:{url_item}\")\n",
    "\n",
    "                if len(urls) > 0:\n",
    "                    f.write(\"\\n\".join(map(lambda s: f\"https://www.plusvalia.com{s}\", urls)))\n",
    "                    urls = []\n",
    "\n",
    "                current_url = self.driver.current_url\n",
    "\n",
    "\n",
    "                if current_url == f\"{self.root}.html\":\n",
    "                    page = 1\n",
    "                else:\n",
    "                    page = int(re.match(r\"^https\\:\\/\\/www\\.plusvalia\\.com\\/[A-z\\-]*(\\d+)\\.html$\", current_url).groups()[0])\n",
    "\n",
    "                if page == page_to:\n",
    "                    break\n",
    "                \n",
    "                # next = self.driver.find_elements(By.CSS_SELECTOR, 'li.pag-go-next > a')[0]\n",
    "                # next.click()\n",
    "                self.wait(waiting_time)\n",
    "\n",
    "        return urls, repeated\n",
    "\n",
    "\n",
    "    def scrape_items(self,urls_items)->list:\n",
    "        \n",
    "        if not self.check_requisites():\n",
    "            return []\n",
    "\n",
    "        \n",
    "    \n",
    "    def close(self):\n",
    "        if self.driver:\n",
    "            self.driver.close()\n",
    "            \n",
    "    def __del__(self):\n",
    "        print(\"Closing...\")\n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapper = ScrapperPlusvalia()\n",
    "# urls, _ = scrapper.collect_urls(1,3468,1.5)  # 3468"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get urls of items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34850\n",
      "16738\n",
      "16737\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "with open(\"urls.txt\") as f:\n",
    "    lines = list(filter(lambda s: \"htmlhttps\" not in s,f.readlines()))\n",
    "    print(len(lines))\n",
    "    lines = np.unique(lines)\n",
    "    print(len(lines))\n",
    "\n",
    "    with open(\"urls_clean.txt\",\"w\") as f_out:\n",
    "        f_out.writelines(lines)\n",
    "\n",
    "with open(\"urls_clean.txt\",\"r\") as f_in:\n",
    "    urls = f_in.readlines()\n",
    "    print(len(urls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemRealState:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        self.type_real_state=\"none\"\n",
    "        self.url=\"none\"\n",
    "        self.title=\"none\"\n",
    "        self.location_string=\"none\"\n",
    "        self.location_latitude=\"none\"\n",
    "        self.location_longitude=\"none\"\n",
    "        self.date_publication=\"none\"\n",
    "        self.state=\"none\"\n",
    "        self.deliver=\"none\"\n",
    "        self.coin=\"none\"\n",
    "        self.views=\"none\"\n",
    "        self.price=0\n",
    "        self.expenses=\"none\"\n",
    "        self.discount=0\n",
    "        self.description_sup=\"none\"\n",
    "        self.description=\"none\"\n",
    "        self.pictures=\"none\"\n",
    "        self.advertiser_name=\"none\"\n",
    "        self.advertiser_codes=\"none\"\n",
    "        self.feature_unities=\"none\"\n",
    "        self.feature_stotal=\"none\"\n",
    "        self.feature_scover=\"none\"\n",
    "        self.feature_bath=\"none\"\n",
    "        self.feature_parking=\"none\"\n",
    "        self.feature_toilete=\"none\"\n",
    "        self.feature_antique=\"none\"\n",
    "        self.feature_rooms=\"none\"\n",
    "        self.general_features_xml=\"none\"\n",
    "        self.location_tree=\"none\"\n",
    "\n",
    "    def extract_first(self, elements, mapping=None, default=\"none\"):\n",
    "        elements_len = len(elements)\n",
    "        if elements_len == 0:\n",
    "            return default        \n",
    "        try:\n",
    "            element = elements[0].text\n",
    "            if mapping is not None:\n",
    "                element = mapping(element)\n",
    "            return element\n",
    "        except:\n",
    "            return default\n",
    "\n",
    "    def extract_multiple(self, web_elements, separator=\"|\"):\n",
    "        \n",
    "        out = \"\"\n",
    "        for i in range(len(web_elements)):\n",
    "            out += web_elements[i].text\n",
    "            if i < len(web_elements)-1:\n",
    "                out += separator        \n",
    "        return out\n",
    "\n",
    "    def get_serie(self, map_attributes_to_columns=dict()):\n",
    "        d = self.getAttributesAsDict()\n",
    "        if len(d) == 0:\n",
    "            print(\"First add atributes with set_attributes\")\n",
    "            return pd.Series()\n",
    "\n",
    "        if len(map_attributes_to_columns) > 0:\n",
    "            # not supported\n",
    "            return pd.Series()\n",
    "        \n",
    "        return pd.Series(data=d)\n",
    "\n",
    "\n",
    "    def parse_attributes(self,scrapper):\n",
    "        today = datetime.date.today()\n",
    "\n",
    "        def dateToString(date):\n",
    "            return f\"{date.day}/{date.month}/{date.year}\"\n",
    "        today_str = dateToString(today)\n",
    "\n",
    "        web_elements_location_tree = scrapper.driver.find_elements(By.XPATH, \"//ul[@class='breadcrumb']/li\")\n",
    "        self.url = scrapper.driver.current_url\n",
    "        self.location_tree = self.extract_multiple(web_elements_location_tree,\"->\")\n",
    "        self.type_real_state = web_elements_location_tree[1].text.strip().lower()\n",
    "\n",
    "        if self.type_real_state != \"proyectos\": # in [\"casa\", \"oficina comercial\",\"departamento\",\"terreno / lote\",\"bodega-galp√≥n\"]:\n",
    "            self.title = self.extract_first(scrapper.xp(\"//hgroup[@class='title-container']//div[@class='section-title']\"))\n",
    "            self.location_string = self.extract_first(scrapper.xp(\"//h2[@class='title-location']\")).replace(\"\\nVer en mapa\", \"\")\n",
    "            url_maps = scrapper.driver.find_elements(By.XPATH, \"//img[@id='static-map']\")[0].get_attribute(\"src\")\n",
    "\n",
    "            raw_lat_lon = re.match(r\".*center=(\\-?\\d+\\.[\\d]+),(\\-?\\d+\\.[\\d]+).*\",url_maps)\n",
    "            if raw_lat_lon:\n",
    "                self.location_latitude, self.location_longitude = map(float,raw_lat_lon.groups())\n",
    "            \n",
    "            self.date_publication = self.extract_first(scrapper.driver.find_elements(By.XPATH, \"//div[@id='user-views']/div/div[1]\"))\n",
    "            self.views = int(self.extract_first(scrapper.driver.find_elements(By.XPATH, \"//div[@id='user-views']/div/div[2]\"),default=\"0 visualizaciones\").split(\" \")[0])\n",
    "            if self.date_publication == \"Publicado hoy\":\n",
    "                self.date_publication = today_str\n",
    "            else:\n",
    "                amount = -1\n",
    "                time_type = \"unknown\"\n",
    "                try:\n",
    "                    gr = re.match(\"Publicado [^\\d]*(\\d+) ([A-z√±√°√©√≠√≥√∫]+)\", self.date_publication).groups()\n",
    "                        \n",
    "                    amount = int(gr[0])\n",
    "                    time_type = gr[1]\n",
    "\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "\n",
    "                if amount >= 0:\n",
    "                    amount = int(amount)\n",
    "                    if \"d√≠a\" in time_type.lower() or \"dia\" in time_type.lower():\n",
    "                        self.date_publication = dateToString(today - datetime.timedelta(days=amount))\n",
    "                    elif \"a√±o\" in time_type:\n",
    "                        self.date_publication = dateToString(today - datetime.timedelta(days=amount*365))\n",
    "                    else:\n",
    "                        self.date_publication = \"unknown\"\n",
    "                        print(f\"Unknown {self.date_publication}\")\n",
    "                else:\n",
    "                    self.date_publication = \"unknown\"\n",
    "\n",
    "            self.state = self.extract_first(scrapper.driver.find_elements(By.XPATH, \"//div[@class='price-operation']\"))\n",
    "            self.deliver = \"house_default\"\n",
    "            price_items_split = self.extract_first(scrapper.driver.find_elements(By.XPATH, \"//div[@class='price-items']/span/span\")).split(\"\\n\")\n",
    "            self.coin, self.price = re.match(\"([A-z]+) ([\\d\\.]+)\", price_items_split[0]).groups()\n",
    "            self.price = float(self.price.replace(\".\",\"\"))\n",
    "            self.discount = 0 if len(price_items_split) == 1 else int(price_items_split[1].replace(\"%\",\"\"))\n",
    "            \n",
    "            self.expenses = self.extract_first(scrapper.xp(\"//div[contains(@class,'block-expensas')]\"))\n",
    "\n",
    "            \n",
    "\n",
    "            self.description_sup = self.extract_first(scrapper.driver.find_elements(By.XPATH, \"//h2[@class='title-type-sup']\")).replace(\"\\n\",\"|\")\n",
    "            self.description = self.extract_first(scrapper.driver.find_elements(By.XPATH, \"//div[@id='longDescription']\")).replace(\"\\n\",\"|\")\n",
    "            \n",
    "            web_elements_imgs = scrapper.driver.find_elements(By.XPATH, \"//div[@id='react-gallery']//img\")\n",
    "            self.pictures = \"\"\n",
    "            for i,element in enumerate(web_elements_imgs):\n",
    "                self.pictures += element.get_attribute(\"src\")\n",
    "                if i < len(web_elements_imgs)-1:\n",
    "                    self.pictures += \"|\"\n",
    "            \n",
    "            self.advertiser_name = self.extract_first(scrapper.driver.find_elements(By.XPATH, \"//section[@id='reactPublisherData']/div/div\")).replace(\"\\n\",\"|\")\n",
    "            self.advertiser_codes = self.extract_multiple(scrapper.driver.find_elements(By.XPATH, \"//section[@id='reactPublisherData']/div/ul/li\")).replace(\"\\n\",\"|\")\n",
    "\n",
    "            features = scrapper.xp(\"//ul[@class='section-icon-features']//li\")\n",
    "            for feature in features:\n",
    "                text = feature.text\n",
    "                if feature.find_elements(By.XPATH, \".//i[@class='icon-stotal']\"):\n",
    "                    self.feature_stotal = text\n",
    "                elif feature.find_elements(By.XPATH, \".//i[@class='icon-scubierta']\"):\n",
    "                    self.feature_scover = text\n",
    "                elif feature.find_elements(By.XPATH, \".//i[@class='icon-bano']\"):\n",
    "                    self.feature_bath = text\n",
    "                elif feature.find_elements(By.XPATH, \".//i[@class='icon-cochera']\"):\n",
    "                    self.feature_parking = text\n",
    "                elif feature.find_elements(By.XPATH, \".//i[@class='icon-toilete']\"):\n",
    "                    self.feature_toilete = text\n",
    "                elif feature.find_elements(By.XPATH, \".//i[@class='icon-antiguedad']\"):\n",
    "                    self.feature_antique = text\n",
    "                elif feature.find_elements(By.XPATH, \".//i[@class='icon-dormitorio']\"):\n",
    "                    self.feature_rooms = text\n",
    "                else:\n",
    "                    i = feature.find_elements(By.XPATH, \".//i\")[0]\n",
    "                    print(f\"unknown {text}\", i.get_attribute(\"class\"), i.text)\n",
    "\n",
    "\n",
    "            general_features = scrapper.xp(\"//div[@id='reactGeneralFeatures']//div[contains(@class,'AccordionItem')]\")\n",
    "            self.general_features_xml = \"<general_features_xml>\"\n",
    "            for feature in general_features:\n",
    "                type_title = feature.find_element(By.XPATH, \".//div[contains(@class,'AccordionHeader')]\").text.strip()\n",
    "                content = feature.find_element(By.XPATH, \".//div[contains(@class,'AccordionContent')]\").text.strip().replace(\"\\n\",\"|\")\n",
    "\n",
    "                element = f\"<general_feature><title>{type_title}</title><content>{content}</content></general_feature>\"\n",
    "                self.general_features_xml += element\n",
    "\n",
    "            self.general_features_xml += \"</general_features_xml>\"\n",
    "\n",
    "\n",
    "            # scrapper.wait(1)\n",
    "\n",
    "        elif self.type_real_state == \"proyectos\":\n",
    "            \n",
    "            self.title = self.extract_first(scrapper.xp(\"//h2[@class='title']\"))\n",
    "            self.location_string = self.extract_first(scrapper.xp(\"//p[@class='subtitle']\")).replace(\"\\nVer en mapa\", \"\")\n",
    "            url_maps = scrapper.driver.find_elements(By.XPATH, \"//img[@id='static-map']\")[0].get_attribute(\"src\")\n",
    "            \n",
    "            raw_lat_lon = re.match(r\".*center=(\\-?\\d+\\.[\\d]+),(\\-?\\d+\\.[\\d]+).*\",url_maps)\n",
    "            if raw_lat_lon:\n",
    "                self.location_latitude, self.location_longitude = map(float,raw_lat_lon.groups())\n",
    "\n",
    "            self.date_publication = self.extract_first(scrapper.driver.find_elements(By.XPATH, \"//div[@id='user-views']/div/div[1]\"))\n",
    "            self.views = int(self.extract_first(scrapper.driver.find_elements(By.XPATH, \"//div[@id='user-views']/div/div[2]\"),default=\"0 visualizaciones\").split(\" \")[0])\n",
    "            if self.date_publication == \"Publicado hoy\":\n",
    "                self.date_publication = today_str\n",
    "            else:\n",
    "                amount = -1\n",
    "                time_type = \"unknown\"\n",
    "                try:\n",
    "                    gr = re.match(\"Publicado [^\\d]*(\\d+) ([A-z√±√°√©√≠√≥√∫]+)\", self.date_publication).groups()\n",
    "                        \n",
    "                    amount = int(gr[0])\n",
    "                    time_type = gr[1]\n",
    "\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "\n",
    "                if amount >= 0:\n",
    "                    amount = int(amount)\n",
    "                    if \"d√≠a\" in time_type.lower() or \"dia\" in time_type.lower():\n",
    "                        self.date_publication = dateToString(today - datetime.timedelta(days=amount))\n",
    "                    elif \"a√±o\" in time_type:\n",
    "                        self.date_publication = dateToString(today - datetime.timedelta(days=amount*365))\n",
    "                    else:\n",
    "                        self.date_publication = \"unknown\"\n",
    "                        print(f\"Unknown {self.date_publication}\")\n",
    "                else:\n",
    "                    self.date_publication = \"unknown\"\n",
    "\n",
    "            self.state = self.extract_first(scrapper.xp(\"/html[1]/body[1]/div[2]/main[1]/div[1]/div[1]/article[1]/div[1]/div[3]/div[2]/div[1]\")).replace(\"Etapa:\",\"\").strip()\n",
    "            self.deliver = self.extract_first(scrapper.xp(\"/html[1]/body[1]/div[2]/main[1]/div[1]/div[1]/article[1]/div[1]/div[3]/div[2]/div[2]\")).replace(\"Entrega:\",\"\").strip()\n",
    "            \n",
    "            price_items_split = self.extract_first(scrapper.driver.find_elements(By.XPATH, \"//div[@class='price-items']\")).split(\"\\n\")\n",
    "            self.coin, self.price = re.match(\"([A-z]+) ([\\d\\.]+)\", price_items_split[0]).groups()\n",
    "            self.price = float(self.price.replace(\".\",\"\"))\n",
    "            self.discount = 0 if len(price_items_split) == 1 else int(price_items_split[1].replace(\"%\",\"\"))\n",
    "            \n",
    "            self.expenses = self.extract_first(scrapper.xp(\"//div[contains(@class,'block-expensas')]\"))\n",
    "\n",
    "            \n",
    "\n",
    "            self.description_sup = self.extract_first(scrapper.driver.find_elements(By.XPATH, \"//h2[@class='title-type-sup']\")).replace(\"\\n\",\"|\")\n",
    "            self.description = self.extract_first(scrapper.driver.find_elements(By.XPATH, \"//div[@id='longDescription']\")).replace(\"\\n\",\"|\")\n",
    "            \n",
    "            web_elements_imgs = scrapper.driver.find_elements(By.XPATH, \"//div[@id='react-gallery']//img\")\n",
    "            self.pictures = \"\"\n",
    "            for i,element in enumerate(web_elements_imgs):\n",
    "                self.pictures += element.get_attribute(\"src\")\n",
    "                if i < len(web_elements_imgs)-1:\n",
    "                    self.pictures += \"|\"\n",
    "            \n",
    "            self.advertiser_name = self.extract_first(scrapper.driver.find_elements(By.XPATH, \"//section[@id='reactPublisherData']/div/div\")).replace(\"\\n\",\"|\")\n",
    "            self.advertiser_codes = self.extract_multiple(scrapper.driver.find_elements(By.XPATH, \"//section[@id='reactPublisherData']/div/ul/li\")).replace(\"\\n\",\"|\")\n",
    "\n",
    "            features = scrapper.xp(\"//div[@class='nf-container']//div[@class='label']\")\n",
    "            for feature in features:\n",
    "                text = feature.text.lower()\n",
    "                \n",
    "                if \"total\" in text:\n",
    "                    self.feature_stotal = text\n",
    "                elif \"unid\" in text:\n",
    "                    self.feature_uni = text\n",
    "                elif \"cubie\" in text:\n",
    "                    self.feature_scover = text\n",
    "                elif \"ba√±o\" in text:\n",
    "                    self.feature_bath = text\n",
    "                elif \"estac\" in text:\n",
    "                    self.feature_parking = text\n",
    "                elif \"habit\" in text:\n",
    "                    self.feature_rooms = text\n",
    "                else:\n",
    "                    print(f\"unknown {text}\")\n",
    "\n",
    "\n",
    "            general_features = scrapper.xp(\"//div[@id='reactGeneralFeatures']//section\")\n",
    "            self.general_features_xml = \"<general_features_xml>\"\n",
    "            for feature in general_features:\n",
    "                type_title = feature.find_element(By.XPATH, \"./div\").text.strip()\n",
    "                content = feature.find_element(By.XPATH, \"./ul\").text.strip().replace(\"\\n\",\"|\")\n",
    "\n",
    "                element = f\"<general_feature><title>{type_title}</title><content>{content}</content></general_feature>\"\n",
    "                self.general_features_xml += element\n",
    "\n",
    "            self.general_features_xml += \"</general_features_xml>\"\n",
    "        else:\n",
    "            print(self.type_real_state, url)            \n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "        return self.getAttributesAsDict()\n",
    "\n",
    "    def getAttributesAsDict(self):\n",
    "        return self.__dict__\n",
    "\n",
    "\n",
    "    def save(self,filename):\n",
    "        with open(filename,\"w\") as file:\n",
    "            json.dump(self.getAttributesAsDict(), file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    def set_attributes(self, dict_attributes:dict):\n",
    "        self.attributes = []\n",
    "        for key,value in dict_attributes.items():\n",
    "            setattr(self, key, value)\n",
    "            self.attributes.append(key)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f22f186c0104fe9ab08cb89ca06d290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=571.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1000 1000\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "df = pd.DataFrame(columns=[\"type_real_state\",\"url\",\"title\",\"location_string\",\"location_latitude\",\"location_longitude\",\"date_publication\",\"state\",\"deliver\",\"coin\",\"views\",\"price\",\"expenses\",\"discount\",\"description_sup\",\"description\",\"pictures\",\"advertiser_name\",\"advertiser_codes\",\"feature_rooms\",\"feature_unities\",\"feature_stotal\",\"feature_scover\",\"feature_bath\",\"feature_parking\",\"feature_toilete\",\"feature_antique\",\"general_features_xml\",\"location_tree\"])\n",
    "index_df = 0\n",
    "s = f\"{datetime.datetime.now()}\"\n",
    "\n",
    "counter = 429\n",
    "counter_end = 1000\n",
    "\n",
    "try:\n",
    "    debug_step = int(counter_end/10)\n",
    "    for i in tqdm(range(counter,counter_end)):\n",
    "        counter = i\n",
    "        url = urls[counter]\n",
    "        scrapper.driver.get(url)\n",
    "        item = ItemRealState()\n",
    "        item.parse_attributes(scrapper)\n",
    "        df.at[index_df, :] = item.get_serie()\n",
    "        index_df += 1\n",
    "        counter += 1\n",
    "        # a.save(f\"temp/{counter}.json\")\n",
    "except Exception as e:\n",
    "    print(\"Error: \", e, counter, url, counter_end)\n",
    "finally:\n",
    "    print(counter,counter_end)\n",
    "    df.to_csv(f\"shards/{s}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oficina comercial https://www.plusvalia.com/propiedades/-0-9-8-2-9-5-3-1-9-3-vendo-oficina-torre-boreal-61870822.html\n",
      "\n",
      "{'casa': 3, 'departamento': 1, 'oficina comercial': 1}\n",
      "counter 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "            \n",
    "\"\"\"\n",
    "self.cache[\"urls\"] = urls_items\n",
    "self.cache[\"index_seen\"] = -1\n",
    "self.cache[\"urls_error\"] = []\n",
    "self.cache[\"df\"] = df\n",
    "\n",
    "\"\"\"\n",
    "import datetime\n",
    "import json\n",
    "##\n",
    "restarting = False\n",
    "cache = dict()\n",
    "stop = False\n",
    "##\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    \n",
    "    for counter in range(0,10):\n",
    "        url = urls[counter]\n",
    "        counter += 1\n",
    "        scrapper.driver.get(url)\n",
    "\n",
    "        \n",
    "        \n",
    "finally:\n",
    "    print(\"counter\", counter)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3c2402a762b1da2b664ca9cbb9344946d41b73132102685c4db1aa6c02b5b44"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
